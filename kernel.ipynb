{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import Counter\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import re\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6331c2d6578d95d38a1afec0a9ac07339a5ee8c7"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(s):\n",
    "    s = re.sub(\"\\n\", \"\", s.lower())\n",
    "    s = re.sub(\"[()$!\\-/\\\\\\\\]\", \"SYMBOL\", s)\n",
    "    s = re.sub(\"\\d{1,5}\", \"NUMBER\", s)\n",
    "    s = re.sub(\"\\.\", \". \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "21f4f22612f1ef1d3f7bb3a5e87cfa2f70844ec1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"yelp_reviews\", \"yelp_review.csv\"), nrows = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8c7366e46d0595acdaaf0f9fae957fc86bfb41f4"
   },
   "outputs": [],
   "source": [
    "df = df[df['stars'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "reviews = df['text'].apply(lambda s: preprocess_text(s))\n",
    "labels = df['stars']\n",
    "\n",
    "label2idx = {l: i for i, l in enumerate(labels.unique())}\n",
    "NUM_LABELS = len(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "486eba092f6814b9041f82a6af1e7e0419b3be6c"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "aa068740cc5c7fe1704a6ae46e6cbaeda5a46894"
   },
   "outputs": [],
   "source": [
    "vocab = Counter([word for review in reviews for word in tokenize.word_tokenize(review)])\n",
    "\n",
    "most_frequent_words = [k for k, c in vocab.items() if c > 5]\n",
    "word2idx = {k: i + 1 for i, k in enumerate(most_frequent_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "ac33a47d6e565561b1a2c3a94a8515c5bc344b20"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx) + 2\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "765e9352d252953df3eeda5995089d511f51dcd3"
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(\"data\", \"embeddings\", \"wiki-news-300d-1M.vec\"), encoding=\"utf8\")\n",
    "contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "60e7db4b7c78b8d8d64043e283f85a14e55f8111"
   },
   "outputs": [],
   "source": [
    "fasttext_words = {}\n",
    "for line in contents[1:]:\n",
    "    line = re.sub(\"\\n\", \"\", line).strip()\n",
    "    line = line.split(\" \")\n",
    "    if word2idx.get(line[0]):\n",
    "        fasttext_words[line[0]] = np.array([float(n) for n in line[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4080dd796d4dc3dd20b71f83ddd84307b4c1505b"
   },
   "outputs": [],
   "source": [
    "embedding_mat = np.empty(shape=(VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in word2idx.items():\n",
    "    if fasttext_words.get(word) is not None:\n",
    "        embedding_mat[idx,:] = fasttext_words[word]\n",
    "    else:\n",
    "        embedding_mat[idx,:] = np.random.uniform(-0.05, 0.05, size = EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "5fde2cec5afe8f17c4accdb7f1c79b606ac37c35"
   },
   "outputs": [],
   "source": [
    "# random initialization for the unknown token\n",
    "embedding_mat[-1,:] = np.random.uniform(-0.05, 0.05, size = EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "dd5307568f3f28c44c189cd7b5920957d5db95cb"
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3910145f8c6ea09d40cc44ede8eb250095924e15"
   },
   "outputs": [],
   "source": [
    "MAX_NUM_SENT = 6\n",
    "MAX_NUM_WORDS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1d5cd20a93f9e39a2ec920157d4cb6b521342382"
   },
   "outputs": [],
   "source": [
    "def review_tokenizer(review, word2idx, num_sent, num_words, unknown_token):\n",
    "    tokenized_reviews = np.zeros((num_sent, num_words), dtype=np.int32)\n",
    "    for n_s, s in enumerate(tokenize.sent_tokenize(review)[:num_sent]):\n",
    "        for n_w, w in enumerate(s.strip().split(\" \")[:num_words]):\n",
    "                tokenized_reviews[n_s, n_w] = word2idx.get(w, unknown_token)\n",
    "    return tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f5631e1c99945123becd31ff2fa5d1734c778645"
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = reviews.apply(lambda r: review_tokenizer(r, word2idx, MAX_NUM_SENT, MAX_NUM_WORDS, VOCAB_SIZE))\n",
    "tokenized_reviews = np.stack(tokenized_reviews.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "02f14653f60ac08f4b8c95d2f46cc5c82890ce2f"
   },
   "outputs": [],
   "source": [
    "multi_class_labels = labels.values - 1\n",
    "binary_labels = labels.apply(lambda x: 1 if x > 3 else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e6ed69bfee7b2e8716c442b61bd0f9f06ec8546a"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "092f52c82f1a3755b3b811ff50fe89128ab76599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, TimeDistributed, Flatten, Dense, Lambda, Reshape, Concatenate, Multiply\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.initializers import Ones, Constant, TruncatedNormal, RandomUniform\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "c9093715476939db7512e4a493a12fe35eeb078d"
   },
   "outputs": [],
   "source": [
    "# Pairwise sentence similarity for full batch\n",
    "def pairwise_dist(x):\n",
    "    new_shape = K.int_shape(x)[2:]\n",
    "    x = K.reshape(x, (-1,) + new_shape)\n",
    "    x1 = K.expand_dims(x, len(new_shape) - 1)\n",
    "    x2 = K.expand_dims(x, len(new_shape))\n",
    "    sq_diff = K.square(x1 - x2)\n",
    "    c = K.sqrt(K.sum(sq_diff, axis = -1))\n",
    "    c = c / (2 * (0.5 ** 2))\n",
    "    sims = K.exp(-c)\n",
    "    return tf.matrix_band_part(sims, -1, 0)\n",
    "\n",
    "# Pairwise prediction difference for full batch\n",
    "def y_derivative(y):\n",
    "    new_shape = K.int_shape(y)[2:]\n",
    "    y = K.reshape(y, (-1,) + new_shape)\n",
    "    y1 = K.expand_dims(y, len(new_shape) - 1)\n",
    "    y2 = K.expand_dims(y, len(new_shape))\n",
    "    sq_diff = K.square(y1 - y2)\n",
    "    return tf.matrix_band_part(sq_diff, -1, 0)\n",
    "\n",
    "# similarity loss function\n",
    "def custom_sim_loss(encoded_reviews, y_hat, batch_size):\n",
    "    sims, pred_sims = pairwise_dist(encoded_reviews), y_derivative(y_hat)\n",
    "    loss = K.sum(K.dot(sims, pred_sims)) / (batch_size ** 2) \n",
    "    return loss\n",
    "\n",
    "# Full custome loss function\n",
    "def custom_loss_wrapper(encoded_reviews, y_hat, batch_size, l, a):\n",
    "    def loss(y_true, y_pred):\n",
    "        ent_loss = binary_crossentropy(y_true, y_pred)\n",
    "        ent_loss = K.reshape(ent_loss, (-1, 1))\n",
    "        sim_loss = custom_sim_loss(encoded_reviews, y_hat, batch_size)\n",
    "        sim_loss = K.reshape(ent_loss, (-1, 1))\n",
    "        return (l * ent_loss) +  (a * sim_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "af59029566f847cfe797fb03698412d152404322"
   },
   "outputs": [],
   "source": [
    "def ConvMax1D(layer, n_layer, ks, padding, activation, dropout_prob):\n",
    "    x = Conv1D(128, ks, padding=\"same\", activation=\"relu\", name=\"channel_{}\".format(n_layer))(layer)\n",
    "    #x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "2f2cf682977e89c2c0d948c52f3250694b8db418"
   },
   "outputs": [],
   "source": [
    "def get_sent_encoder(max_num_words, max_num_sent, vocab_size, dropout_prob, embedding_dim, embedding_mat, embedding_trainable):\n",
    "    sent = Input((max_num_words,), name=\"sent_input\")\n",
    "    embed = Embedding(vocab_size, embedding_dim, weights=[embedding_mat], trainable=embedding_trainable, name=\"sent_embed\")(sent)\n",
    "    channels = [ConvMax1D(embed, i, ks, \"same\", \"relu\", dropout_prob) for i, ks in enumerate([2,3,4,5,6])]\n",
    "    x = Concatenate()(channels)\n",
    "    x = Conv1D(128, 3, padding = \"same\", activation=\"relu\")(x)\n",
    "    #x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    x = Conv1D(256, 3, padding = \"same\")(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    x = Conv1D(512, 3, padding = \"same\")(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    x = Conv1D(512, 3, padding = \"same\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(2048, activation = \"relu\")(x)\n",
    "    x = Dense(2048, activation = \"relu\")(x)\n",
    "    x = Dropout(dropout_prob)(x)\n",
    "    sent_encode = x\n",
    "    \n",
    "    sent_encoder = Model(inputs=sent, outputs=sent_encode)\n",
    "    return sent_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "b13739bcbf35d6142d40ce3282c05e443424f9ea"
   },
   "outputs": [],
   "source": [
    "def get_model(sent_encoder, max_num_words, max_num_sent, dropout_prob):\n",
    "    review = Input((max_num_sent, max_num_words))\n",
    "    mask = Input((max_num_sent,1))\n",
    "    \n",
    "    encoded_reviews = TimeDistributed(sent_encoder)(review)\n",
    "    encoded_reviews = Dropout(dropout_prob)(encoded_reviews)\n",
    "    \n",
    "    # predictionson sentence sentiments\n",
    "    y_hat = Dense(1, activation=\"sigmoid\", name=\"sent_sentiment\")(encoded_reviews)\n",
    "    y_hat = Multiply(name=\"masked_sent_sentiment\")([y_hat, mask])\n",
    "\n",
    "    sent_avg_out = Lambda(lambda x: K.sum(x[0], axis=[-2]) / K.sum(x[1], axis=[-2]), name=\"sent_agg_pred\")([y_hat, mask])\n",
    "    \n",
    "    #x = Dense(512, activation=\"relu\")(encoded_reviews)\n",
    "    #x = Dense(1024, activation=\"relu\")(x)\n",
    "    #x = Flatten()(x)\n",
    "    #multi_class_out = Dense(5, activation=\"softmax\")(x)\n",
    "    model = Model(inputs = [review, mask], outputs = sent_avg_out)\n",
    "    \n",
    "    return model, encoded_reviews, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "bb323008d46e20c49bce910acfe6ff55b2efbf02"
   },
   "outputs": [],
   "source": [
    "sent_encoder = get_sent_encoder(MAX_NUM_WORDS, MAX_NUM_SENT, VOCAB_SIZE, 0.3, EMBEDDING_DIM, embedding_mat, False)\n",
    "\n",
    "model, encoded_reviews, y_hat = get_model(sent_encoder, MAX_NUM_WORDS, MAX_NUM_SENT, 0.4)\n",
    "\n",
    "sent_sentiment_layer = model.get_layer(\"masked_sent_sentiment\")\n",
    "sent_sentiment_model = Model(inputs = model.input, \n",
    "                             outputs = sent_sentiment_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f7435238b56599ce691d96673a75d3303e80e690"
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(),\n",
    "              loss=custom_loss_wrapper(encoded_reviews, y_hat, BATCH_SIZE, 1, 0),\n",
    "              #loss_weights = [0, 1],\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "335e7445494f289b508251192703d542734a8dad"
   },
   "outputs": [],
   "source": [
    "mask_mat = np.sum(tokenized_reviews,axis=-1).reshape(-1, MAX_NUM_SENT, 1)\n",
    "mask_mat[mask_mat > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "57339284f0fb983dd9d2b6d3ab6a21bd370325ab"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "e80462f6a6828280fecf6770854a8af4ddc7f326"
   },
   "outputs": [],
   "source": [
    "def decaying_lr(epoch):\n",
    "    initial_lr = 0.001\n",
    "    return initial_lr / (2 ** np.floor(epoch / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "218d6426409363e16a5878a8499ce1f2fad4e213"
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(decaying_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "34a453e76d1a90ae3bb569b1ea1c49882580b4f8"
   },
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ececb9ad1005522c980221c14df9d5f999f03d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70553 samples, validate on 17639 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit([tokenized_reviews, mask_mat], \n",
    "          binary_labels, BATCH_SIZE, validation_split=0.2, epochs = 10, callbacks=[earlystopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30a42ff081e906ccbc4a95f30726b4339fd433ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_sentiment_pred = sent_sentiment_model.predict([tokenized_reviews, mask_mat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac03a1c5ebd1a07ceccff88627f71964347e49a8",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(170, 180, 1):\n",
    "    print(\"Review ratings: {}\".format(labels.iloc[i]))\n",
    "    for s_i, sent in enumerate(tokenize.sent_tokenize(reviews.iloc[i])[:MAX_NUM_SENT]):\n",
    "        sent_pred = sent_sentiment_pred[i][s_i]\n",
    "        \n",
    "        if sent_pred == 0:\n",
    "            break\n",
    "        elif sent_pred > 0.7:\n",
    "            print('\\033[1;42m{}\\033[1;m'.format(sent))\n",
    "        elif sent_pred < 0.3:\n",
    "            print('\\033[1;41m{}\\033[1;m'.format(sent))\n",
    "        else:\n",
    "            print('\\033[1;47m{}\\033[1;m'.format(sent))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "153d1662dee9dbe60eb169cf3353c9c4f25fbff5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28d3946c4c992f366c15f0f54d6d3fc46e5a6b78",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
